{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXnsOLEDxIPWERYDzHbhaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akmadiha27/Building-LLMS-From-scratch/blob/main/Tokenizer_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading raw text from the file"
      ],
      "metadata": {
        "id": "0vX63_OwaDI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yIKeeT_5YUpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a4ef1a-8a92-40ff-b504-37f49954d130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of characters are: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open (\"/content/the-verdict (1).txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_txt=f.read()\n",
        "print(\"The number of characters are:\", len(raw_txt))\n",
        "print(raw_txt[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the text to make tokens and remove whitespaces also!!\n",
        "Whitespace -> reduces memory but if code sensible to structure then we need to keep it.\n",
        "Question marks, quotations we need all as seperate tokens"
      ],
      "metadata": {
        "id": "dJefoYuqZ6lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n"
      ],
      "metadata": {
        "id": "9ZOItJqTYsyZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_txt)\n",
        "preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF7_MUAHdCEi",
        "outputId": "f0c85620-afbf-43b6-b3f9-e76f55d0bc72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MMHkk2ygT5p",
        "outputId": "d3ea5fa2-0801-4126-c2d6-14f84b96e319"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting tokens into token ids so that they have numerical representation"
      ],
      "metadata": {
        "id": "NodYELhDgdnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will se how many words are there"
      ],
      "metadata": {
        "id": "jWB3pK98kjQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  all_words=sorted(set(preprocessed))\n",
        "  print(len(all_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEhw6GNcgXPW",
        "outputId": "5e26ab05-e2d0-47cb-a62e-92f65bcc1e6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Har ek token ko ek number dere ab yahan"
      ],
      "metadata": {
        "id": "eYwonLA3lRmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "8KV2IOhHki4_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>=20:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56vqWhjflBCW",
        "outputId": "4bef5016-f705-464c-a19f-f0217b86c0c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reverse dictionary to be made so that we have word from id"
      ],
      "metadata": {
        "id": "ah8NMOImmowL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SIMPLE TOKENIZER CLASS IN PYTHON**"
      ],
      "metadata": {
        "id": "n-1VJAZUm54f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int=vocab\n",
        "    self.int_to_str={i:s for s,i in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
        "    ids=[self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "  def decode(self,ids):\n",
        "    text=\" \".join([self.int_to_str[i]for i in ids])\n",
        "    #replacing spaces before specified punctuations\n",
        "    text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "VlevQ94dlOsk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cQ6nMINpBDZ",
        "outputId": "3f11f6ba-41d7-4483-af9d-cdde20a5e689"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting back to original set"
      ],
      "metadata": {
        "id": "XLlrz0uHt-yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GM80w6sat1z4",
        "outputId": "05ab9f17-331c-44c3-c016-8b59d2af8028"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note that if particular text is not there in the vocab then it gives error*"
      ],
      "metadata": {
        "id": "eE9WCTOEuF4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ADDING SPECIAL CONTEXT TOKENS**"
      ],
      "metadata": {
        "id": "9K2kqQqbwAWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens=sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
        "\n",
        "vocab={token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "Kmo040Nlv_Zw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkcRsai7uBy1",
        "outputId": "a9befecd-ae3e-484e-90ac-d5e475742730"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hl4T3Ov61C0",
        "outputId": "bd7c8a73-add1-4b91-8204-f5f8806f7cae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int=vocab\n",
        "    self.int_to_str={i:s for s,i in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
        "    #new thing is add unknown\n",
        "    preprocessed=[item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids=[self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "  def decode(self,ids):\n",
        "    text=\" \".join([self.int_to_str[i]for i in ids])\n",
        "    #replacing spaces before specified punctuations\n",
        "    text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "hWuVXtrK7-K-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=SimpleTokenizerV2(vocab)\n",
        "text1=\"Hello, do you like tea?\"\n",
        "text2=\"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text=\"<|endoftext|>\".join((text1,text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blm4RRs-8l4E",
        "outputId": "4b6ca1ad-0476-44c4-b0a7-b29ce2eacdcd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utuJwVKR9D1y",
        "outputId": "3236074f-189e-46fc-afb5-4b3de50365fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oYA6EhNp9wIa",
        "outputId": "cfdab5d3-4c37-4f1d-f6c5-7ae107615ca1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|unk|> the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we also have other tokens like BOS,EOS,PAD"
      ],
      "metadata": {
        "id": "34_qE8yP-Ii7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT uses only endoftext tokens | unk is also not used, it does byte pair encoding."
      ],
      "metadata": {
        "id": "xoVe8Kj4-1C0"
      }
    }
  ]
}